
## Where the samples will be written
save_data: opennmt/wikiedits/artificial-english/combined/6_4_ratio/finetuned/with-wikiedits/data

# Vocabulary files, generated by onmt_build_vocab
src_vocab: opennmt/wikiedits/artificial-english/combined/6_4_ratio/finetuned/with-wikiedits/data/vocab.src
tgt_vocab: opennmt/wikiedits/artificial-english/combined/6_4_ratio/finetuned/with-wikiedits/data/vocab.tgt


# max length was 151
src_seq_length: 160
tgt_seq_length: 160



# Training files
data:
    corpus_1:
        path_src: data/wikiedits/train.bpe.src
        path_tgt: data/wikiedits/train.bpe.tgt
        transforms: [filtertoolong]
    valid:
        path_src: data/wikiedits/valid.bpe.src
        path_tgt: data/wikiedits/valid.bpe.tgt
        transforms: [filtertoolong]



# General opts
log_file: opennmt/wikiedits/artificial-english/combined/6_4_ratio/finetuned/with-wikiedits/checkpoint/log.txt
save_model: opennmt/wikiedits/artificial-english/combined/6_4_ratio/finetuned/with-wikiedits/checkpoint/model_6E6D
tensorboard_log_dir: opennmt/wikiedits/artificial-english/combined/6_4_ratio/finetuned/with-wikiedits/checkpoint/tensorboard
tensorboard: true


average_decay: 0.0005
seed: 42
report_every: 10




# batch size 32000, per sentence tokens 160, so 1 batch will contain  = 200
# total sentences = 600000 so total batches = 600000/200  = 3000 


train_from: opennmt/wikiedits/artificial-english/combined/6_4_ratio/checkpoint/model_6E6D_step_21000.pt

train_steps: 100000

# validate every
valid_steps: 25

save_checkpoint_steps: 25

# early stopping
early_stopping: 5 

keep_checkpoint: 10





# # Number of GPUs, and IDs of GPUs
world_size: 1
gpu_ranks: [0]

# Batching
bucket_size: 262144
num_workers: 2  # Default: 2, set to 0 when RAM out of memory
batch_type: "tokens"
batch_size: 32000   # Tokens per batch, change when CUDA out of memory
valid_batch_size: 2048
max_generator_batches: 2
# accum_count: [2]
# accum_steps: [0]

# Optimization
model_dtype: "fp32"
optim: "adam"
learning_rate: 1
# Default: 4000 - for large datasets, try up to 8000
warmup_steps: 1000
decay_method: "noam"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.1
param_init: 0
param_init_glorot: true
normalization: "tokens"

# Model
encoder_type: transformer
decoder_type: transformer
position_encoding: true
enc_layers: 6
dec_layers: 6
heads: 8
hidden_size: 512
word_vec_size: 512
transformer_ff: 1024
dropout_steps: [0]
dropout: [0.1]
attention_dropout: [0.1]
